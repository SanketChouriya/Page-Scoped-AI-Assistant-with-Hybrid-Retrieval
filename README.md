
# Page-Scoped AI Assistant with Hybrid Retrieval

A **page-scoped AI assistant** that answers user questions using **only the content of the currently viewed webpage**.  
The system combines **PostgreSQL full-text search** with **semantic retrieval** to maximize **accuracy** while minimizing **latency**.

This project demonstrates a **real-world RAG architecture**, not a toy chatbot.

---

## ğŸš€ Key Capabilities

- Page-scoped context (no cross-page leakage)
- Hybrid retrieval (keyword + semantic)
- Low-latency responses
- Strict grounding (no hallucinations)
- Chrome extension integration
- Graceful handling of unknown questions

---

## ğŸ§  Why Page-Scoped AI?

Traditional chatbots:
- Use global knowledge
- Hallucinate answers
- Are slow and unfocused

This system:
- Answers **only from the current webpage**
- Rejects out-of-context questions
- Keeps LLM context small and precise

---

## ğŸ— Architecture Overview

```

Browser (Chrome Extension)
â†“
Content Ingestion API (Django)
â†“
PostgreSQL (Full-Text Search Index)
â†“
Hybrid Retrieval Layer
â†“
LLM (Context-Bound Answer)

```

---

## ğŸ” Hybrid Retrieval Strategy

### 1ï¸âƒ£ Keyword Search (PostgreSQL Full-Text)
- Exact term matching
- Very low latency (milliseconds)
- Ideal for factual content (policies, definitions, specs)

### 2ï¸âƒ£ Semantic Retrieval (Embeddings)
- Handles paraphrased or conceptual queries
- Used only when keyword results are insufficient

### 3ï¸âƒ£ LLM Answering
- Receives **only retrieved content**
- Enforced grounding to prevent hallucinations

This approach balances **precision**, **recall**, and **speed**.

---

## ğŸ§ª Example Query Flow

**User question:**  
> â€œWhat is the return policy?â€

**System behavior:**
1. Postgres FTS finds policy-related sections
2. Semantic reranking selects best match
3. LLM answers using only that context

**Out-of-context query:**  
> â€œWhat is React?â€

**Response:**  
> â€œI don't know based on the provided context.â€

---

## âš™ï¸ Tech Stack

- **Backend:** Django 5.x
- **Database:** PostgreSQL (Full-Text Search)
- **Retrieval:** Hybrid (Keyword + Semantic)
- **LLM:** OpenAI (context-bound)
- **Frontend:** Chrome Extension (Manifest v3)
- **Vector Store:** In-memory (demo-optimized)

---

## ğŸ›¡ Guardrails & Safety

- Page-scoped session isolation
- Strict prompt grounding
- Out-of-context detection
- Content size limits
- Graceful invalid session handling

---

## â± Accuracy & Latency Focus

- Keyword retrieval: ~5â€“20 ms
- Semantic fallback: ~50â€“150 ms
- LLM call minimized by tight context
- Reduced hallucinations via strict retrieval

---

## âŒ Why Not Fine-Tuning?

- Page content is dynamic
- Session-scoped data
- RAG is cheaper, faster, and more reliable

---

## ğŸ“¦ Project Structure

```

apps/
â”œâ”€â”€ content/      # Page ingestion & Postgres FTS
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ serializers.py
â”‚   â”œâ”€â”€ views.py
â”‚   â”œâ”€â”€ search.py
â”‚   â””â”€â”€ urls.py
â”‚
â”œâ”€â”€ ai/           # Retrieval & reasoning
â”‚   â”œâ”€â”€ vector_store.py
â”‚   â”œâ”€â”€ retrieval.py
â”‚   â”œâ”€â”€ llm.py
â”‚   â”œâ”€â”€ serializers.py
â”‚   â”œâ”€â”€ views.py
â”‚   â””â”€â”€ urls.py

```

---

## ğŸŒ Chrome Extension

The Chrome extension:
- Extracts visible webpage content
- Sends it to the ingestion API
- Allows users to ask page-specific questions

This turns the system into a **real, usable product**.
